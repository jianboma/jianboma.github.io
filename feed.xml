<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://jianboma.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jianboma.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-21T10:06:45+00:00</updated><id>https://jianboma.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Quick Paper Post</title><link href="https://jianboma.github.io/blog/2023/quick-paper-share/" rel="alternate" type="text/html" title="Quick Paper Post"/><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>https://jianboma.github.io/blog/2023/quick-paper-share</id><content type="html" xml:base="https://jianboma.github.io/blog/2023/quick-paper-share/"><![CDATA[<p>This post will collect papers that appears to be interesting, but without careful reading.</p> <h2 id="2023-12-12">2023-12-12</h2> <p><a href="https://arxiv.org/abs/2309.14405">Joint Audio and Speech Understanding</a><br/> <a href="https://github.com/YuanGongND/ltu">github_link</a></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{gong_ltuas,
  title={Joint Audio and Speech Understanding},
  author={Gong, Yuan and Liu, Alexander H and Luo, Hongyin, and Karlinsky, Leonid and Glass, James},
  year={2023},
  booktitle={2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
}
</code></pre></div></div> <details> <summary>Details</summary> <b>Authors</b>: Yuan Gong, Alexander H. Liu, Hongyin Luo, Leonid Karlinsky, James Glass <br/> <b>Date</b>: 25 Sep 2023 <br/> <b>#Citations</b>: <br/> <div class="col-sm mt-3 mt-md-0"> The figure is from https://github.com/YuanGongND/ltu. All rights reserved to its owner. <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/quick-paper-share/ltu-overview-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/quick-paper-share/ltu-overview-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/quick-paper-share/ltu-overview-1400.webp"/> <img src="/assets/img/quick-paper-share/ltu-overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </details> <p><a href="https://arxiv.org/pdf/2111.00396.pdf">Efficiently Modeling Long Sequences with Structured State Spaces</a><br/></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{gu2021efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2111.00396},
  year={2021}
}
</code></pre></div></div> <h2 id="2024-01-22">2024-01-22</h2> <p><a href="https://openreview.net/pdf?id=lJkOCMP2aW">Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting</a><br/> <a href="N/A">github_link</a><br/></p> <p><strong>Category</strong>: transformer variation, multi-scale, time series</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{32897a63141342c1a067d56df134df8a,
title = "Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting",
abstract = "Transformer-based models have achieved significant success in time series forecasting. Existing methods mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. In this paper, we propose multi-scale transformers with adaptive pathways (Pathformer). The proposed Transformer integrates both temporal resolution and temporal distance for multi-scale modeling. Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies. We further enrich the multi-scale transformer with adaptive pathways, which adaptively adjust the multi-scale modeling process based on the varying temporal dynamics in the input time series, improving the prediction accuracy and generalization of Pathformer. Extensive experiments on nine real-world datasets demonstrate that Pathformer not only achieves state-of-the-art performance by surpassing all current models but also exhibits stronger generalization abilities under various transfer scenarios.",
author = "Peng Chen and Yingying Zhang and Yunyao Cheng and Yang Shu and Yihang Wang and Qingsong Wen and Bin Yang and Chenjuan Guo",
year = "2024",
month = jan,
day = "16",
language = "English",
booktitle = "International Conference on Learning Representations",
}
</code></pre></div></div> <details> <summary>Details</summary> <b>Authors</b>: <br/> <b>Date</b>: ICLR 2024 <br/> <b>#Citations</b>: <br/> <div class="col-sm mt-3 mt-md-0"> The figure is from https://openreview.net/pdf?id=lJkOCMP2aW. All rights reserved to its owner. <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/quick-paper-share/peng2024multiscale-nn-structure-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/quick-paper-share/peng2024multiscale-nn-structure-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/quick-paper-share/peng2024multiscale-nn-structure-1400.webp"/> <img src="/assets/img/quick-paper-share/peng2024multiscale-nn-structure.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </details> <p><br/> <br/> <a href="https://arxiv.org/abs/2401.09417">Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model</a><br/> <a href="https://github.com/hustvl/Vim">github_link</a><br/></p> <p><strong>Category</strong>: mamba for vision, representation learning, efficient model</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{zhu2024vision,
  title={Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model},
  abstract = "Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., Mamba, have shown great potential for long sequence modeling. Building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance of visual representation learning on self-attention is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation &amp; memory efficiency. For example, Vim is 2.8× faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248×1248. The results demonstrate that Vim is capable of overcoming the computation &amp; memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to become the next-generation backbone for vision foundation models. ",
  author={Zhu, Lianghui and Liao, Bencheng and Zhang, Qian and Wang, Xinlong and Liu, Wenyu and Wang, Xinggang},
  journal={arXiv preprint arXiv:2401.09417},
  year={2024}
}
</code></pre></div></div> <details> <summary>Details</summary> <b>Authors</b>: <br/> <b>Date</b>: Jan 2024 <br/> <b>#Citations</b>: <br/> <div class="col-sm mt-3 mt-md-0"> The figure is from the paper. All rights reserved to its owner. <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/quick-paper-share/zhu2024vision-overview-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/quick-paper-share/zhu2024vision-overview-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/quick-paper-share/zhu2024vision-overview-1400.webp"/> <img src="/assets/img/quick-paper-share/zhu2024vision-overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </details> <p><br/> <br/> <a href="https://arxiv.org/abs/2401.09417">VMamba Visual State Space Model</a><br/> <a href="https://github.com/MzeroMiko/VMamba">github_link</a><br/></p> <p><strong>Category</strong>: mamba for vision, representation learning, efficient model</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{liu2024vmamba,
  title={VMamba: Visual State Space Model},
  author={Liu, Yue and Tian, Yunjie and Zhao, Yuzhong and Yu, Hongtian and Xie, Lingxi and Wang, Yaowei and Ye, Qixiang and Liu, Yunfan},
  journal={arXiv preprint arXiv:2401.10166},
  year={2024}
}
</code></pre></div></div> <details> <summary>Details</summary> <b>Authors</b>: <br/> <b>Date</b>: Jan 2024 <br/> <b>#Citations</b>: <br/> <div class="col-sm mt-3 mt-md-0"> The figure is from the paper. All rights reserved to its owner. <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/quick-paper-share/zhu2024vision-overview-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/quick-paper-share/zhu2024vision-overview-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/quick-paper-share/zhu2024vision-overview-1400.webp"/> <img src="/assets/img/quick-paper-share/zhu2024vision-overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </details> <p><br/> <br/> <a href="https://arxiv.org/abs/2301.12597">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a><br/> <a href="https://github.com/salesforce/LAVIS/tree/main/projects/blip2">github_link</a><br/></p> <p><strong>Category</strong>: vision language model</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}
</code></pre></div></div> <details> <summary>Details</summary> <b>Authors</b>: <br/> <b>Date</b>: Jun 2023 <br/> <b>#Citations</b>: <br/> <div class="col-sm mt-3 mt-md-0"> The figure is from the paper. All rights reserved to its owner. <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/quick-paper-share/li2023blip-overview-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/quick-paper-share/li2023blip-overview-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/quick-paper-share/li2023blip-overview-1400.webp"/> <img src="/assets/img/quick-paper-share/li2023blip-overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </details> <p><br/></p> <h2 id="2024-02-04">2024-02-04</h2> <p><a href="https://allenai.org/olmo/olmo-paper.pdf">OLMo Accelerating the Science of Language Models</a><br/> <a href="https://github.com/allenai/OLMo">github_link</a><br/></p> <p><strong>Category</strong>: Large language model, open-source</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Technical report
</code></pre></div></div> <details> <summary>Details</summary> <b>Authors</b>: <br/> <b>Date</b>: Feb 2024 <br/> <b>#Citations</b>: <br/> <div class="col-sm mt-3 mt-md-0"> The figure is from https://github.com/allenai/OLMo. All rights reserved to its owner. <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/quick-paper-share/olmo-icon-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/quick-paper-share/olmo-icon-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/quick-paper-share/olmo-icon-1400.webp"/> <img src="/assets/img/quick-paper-share/olmo-icon.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </details>]]></content><author><name></name></author><category term="paper_read"/><category term="quick_paper_share"/><summary type="html"><![CDATA[a collection of papers without careful reading]]></summary></entry></feed>