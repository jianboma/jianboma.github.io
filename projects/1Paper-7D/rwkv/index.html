<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Read `RWKV Reinventing RNNs for the Transformer Era` | Jianbo Ma</title> <meta name="author" content="Jianbo Ma"> <meta name="description" content="the icon is from https://wiki.rwkv.com/, all rights reserves to its onwer."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/profile_head3.png?827aece6b42572b4a67200cce7342aee"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jianboma.github.io/projects/1Paper-7D/rwkv/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Read `RWKV Reinventing RNNs for the Transformer Era`",
      "description": "the icon is from https://wiki.rwkv.com/, all rights reserves to its onwer.",
      "published": "December 16, 2023",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jianbo </span>Ma</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/blog/">Blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Read `RWKV Reinventing RNNs for the Transformer Era`</h1> <p>the icon is from https://wiki.rwkv.com/, all rights reserves to its onwer.</p> </d-title><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#d1">D1</a></div> <ul> <li><a href="#tl-dr">TL;DR</a></li> <li><a href="#quadratic-complexity">Quadratic complexity</a></li> <li><a href="#background">Background</a></li> </ul> <div><a href="#d2">D2</a></div> <ul> <li><a href="#time-mixing-and-channel-mixing">Time mixing and channel mixing</a></li> </ul> <div><a href="#d3">D3</a></div> <div><a href="#d5">D5</a></div> <div><a href="#d6">D6</a></div> </nav> </d-contents> <h1 id="rwkv-reinventing-rnns-for-the-transformer-era">RWKV: Reinventing RNNs for the Transformer Era</h1> <p>Paper link: <a href="https://arxiv.org/abs/2305.13048" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2305.13048</a><br> <strong>Homepage</strong>: <a href="https://wiki.rwkv.com/" rel="external nofollow noopener" target="_blank">https://wiki.rwkv.com/</a> <br> There are many projects inside the homepage.</p> <h2 id="d1">D1</h2> <h3 id="tldr">TL;DR</h3> <p>Transformer suffers from memory and computational complexity that scales quadratically with sequence length. Recurrent neural networks (RNNs) has linear scaling in memory and computational requirements but the recurrent mechanism prevents the parallelization and scalability. The proposed Receptance Weighted Key Value (RWKV) <d-cite key="peng2023rwkv"></d-cite> intended to combine the efficient parallelization and scalability and reserve the efficient inference of RNNs.</p> <h3 id="quadratic-complexity">Quadratic complexity</h3> <p>In self-attention mechanism, the Key \(\mathbf{k}\), Query \(\mathbf{q}\) and Value \(\mathbf{v}\) are of length \(\mathrm{T}\). The attention operator can be written as:</p> <p> \begin{equation} \label{eq:vanilla-self-attn} \mathrm{Attn(Q, K, V)}_{t}= \frac{\sum_{i=1}^{T} e^{q_{t}^{\intercal} k_{i}}v_{i}}{\sum_{i=1}^{T} e^{q_{t}^{\intercal} k_{i}}} \end{equation} </p> <p>It used the same idea in Attention Free Transformer (AFT), the alternative formulation is,</p> <p> \begin{equation} \label{eq:aft-attn} \mathrm{Attn^{+}(W, K, V)}_{t}= \frac{\sum_{i=1}^{T} e^{w_{t,i}+ k_{i}}v_{i}}{\sum_{i=1}^{T} e^{w_{t,i}+ k_{i}}}, \end{equation} </p> <p>where \(w_{t,i} \in R^{T \times T}\) is kind of offset learned during training and each \(w_{t,i}\) is a number.</p> <p>The RWKV makes it simpler, instead of learn the matrix offset in AFT, it learns a vector of \(d\) dimensions, where \(d\) is the number of channel (I suppose they mean the dimension of input vetor of RWKV). It then multiply the relative position, makes the offset is expressed as,</p> <p> \begin{equation} \label{eq:rwkv-attn-offset} w_{t,i}=-(t-i)w, \end{equation} </p> <p>where \(w \in ({R_{\geqslant 0}}^{d})\). This means the learned parameter in the offset is a vector rather than a matrix in AFT.</p> <h3 id="background">Background</h3> <p>They presented the RNNs, especially LSTMs.</p> <p>The RWKV seems to be inspired by the Attention Free Transformer (AFT), where learned pair-wise position biases in attention weights are learned. In this way, AFT skip the dot-product and does not need the query.</p> <p>The RWKV used the same mechanism of AFT, but instead of linear basise, a decayed bais is used.</p> <p> <details> <summary>Details in paper</summary> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-attention-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-attention-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-attention-1400.webp"></source> <img src="/assets/img/1Paper-7D/rwkv/rwkv-attention.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </details> </p> <p>Some background from AFT. In AFT, there are still Query, Key and Value matrix. Instead of using dot-product between queries and keys, it learns an offset matrix and add an offset scaler to keys to form the attention weights. <mark>The computational advantage then relies on the removal of dot-product, which seems to be not that appealing.</mark></p> <p> <details> <summary>Details in paper</summary> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/rwkv/aft-attn-eq-explain-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/rwkv/aft-attn-eq-explain-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/rwkv/aft-attn-eq-explain-1400.webp"></source> <img src="/assets/img/1Paper-7D/rwkv/aft-attn-eq-explain.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/rwkv/aft-attn-eq-illustration-fig-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/rwkv/aft-attn-eq-illustration-fig-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/rwkv/aft-attn-eq-illustration-fig-1400.webp"></source> <img src="/assets/img/1Paper-7D/rwkv/aft-attn-eq-illustration-fig.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </details> </p> <h2 id="d2">D2</h2> <h3 id="time-mixing-and-channel-mixing">Time mixing and channel mixing</h3> <p>The time mixing is a core claim of the RWKV and it is not clear what is it and how it is achieved and what are the reasonings behind from <a href="#d1">D1</a>.</p> <blockquote> <p>The recurrence is formulated both as a linear interpolation between the current input and the input at the previous time step (a technique we refer to as time-shift mixing or token shift, indicated by the diagonal lines in Fig. 3)</p> </blockquote> <p>This suggests that all of the \(\mathbf{k}\), \(\mathbf{v}\) are interpolated by current time step and previous time step. But there is not <strong>state</strong>.</p> <p> <details> <summary>Figure in paper</summary> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-architecture-for-language-modelling-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-architecture-for-language-modelling-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-architecture-for-language-modelling-1400.webp"></source> <img src="/assets/img/1Paper-7D/rwkv/rwkv-architecture-for-language-modelling.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </details> </p> <h2 id="d3">D3</h2> <p>The time-mixing block equation is as below,</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-time-mixing-eqs-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-time-mixing-eqs-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-time-mixing-eqs-1400.webp"></source> <img src="/assets/img/1Paper-7D/rwkv/rwkv-time-mixing-eqs.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-channel-mixing-eqs-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-channel-mixing-eqs-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-channel-mixing-eqs-1400.webp"></source> <img src="/assets/img/1Paper-7D/rwkv/rwkv-channel-mixing-eqs.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>This \(wkv_{t}\) is summation like attention mechanism. Each receptance \(r_{t}\), key \(k_{t}\) is interpolation of current time step and previous time step.</p> <p><strong>Qeustion</strong>:</p> <ol> <li>what is \(u\) in equation 14? <br> </li> <li>Why are equation 16-18 called <code class="language-plaintext highlighter-rouge">channel-mixing</code> ?<br> </li> <li>What are the intuition behind the <code class="language-plaintext highlighter-rouge">time-mixing</code> and followed by <code class="language-plaintext highlighter-rouge">channel-mixing</code>?<br> </li> <li>How to make them recursively running during inference?</li> </ol> <details> <summary>answer in paper</summary> Q4: <div class="col-sm mt-3 mt-md-0"> An interesting perspective in paper, time-mixing block as an RNN cell. <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-as-rnn-cell-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-as-rnn-cell-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-as-rnn-cell-1400.webp"></source> <img src="/assets/img/1Paper-7D/rwkv/rwkv-as-rnn-cell.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </details> <p><br></p> <h2 id="d5">D5</h2> <p>Questions:</p> <ul> <li>Why can the <code class="language-plaintext highlighter-rouge">RWKV</code> be trained parallelly but not RNNs? What makes <code class="language-plaintext highlighter-rouge">RWKV</code> special? Does this result in some kind of trade-off? <ul> <li>Equation 11-13, equation 16-17 do not contain non-linear operator, which means \(\mu_{r}, \mu_{k}\) and \(\mu_{v}\) are parameters that can be learned or pre-defined as hyper-parameters. <strong>Does not this like a filter in CNNs</strong>? As there is no non-linearity, the operators can be merged. Additionally, there is no contextual state, e.g. <code class="language-plaintext highlighter-rouge">c</code>, that is updated for each steps.</li> </ul> </li> </ul> <h2 id="d6">D6</h2> <p>Try some codings.</p> <p>Both following are good places to start with <a href="https://ben.bolte.cc/rwkv-model" rel="external nofollow noopener" target="_blank">https://ben.bolte.cc/rwkv-model</a>, <a href="https://johanwind.github.io/2023/03/23/rwkv_details.html" rel="external nofollow noopener" target="_blank">https://johanwind.github.io/2023/03/23/rwkv_details.html</a>.</p> <p>The one from Ben’s blog can be found on his github page <a href="https://github.com/codekansas/rwkv" rel="external nofollow noopener" target="_blank">https://github.com/codekansas/rwkv</a>.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/for_rwkv.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Jianbo Ma. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>