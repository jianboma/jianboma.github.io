<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Masked Audio Generation using a Single Non-Autoregressive Transformer | Jianbo Ma</title> <meta name="author" content="Jianbo Ma"> <meta name="description" content="Figure is from https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT, all rights reserve to its owner"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/profile_head3.png?827aece6b42572b4a67200cce7342aee"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jianboma.github.io/projects/1Paper-7D/2024-01-31-magnet/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Masked Audio Generation using a Single Non-Autoregressive Transformer",
      "description": "Figure is from https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT, all rights reserve to its owner",
      "published": "January 31, 2024",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jianbo </span>Ma</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/blog/">Blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Masked Audio Generation using a Single Non-Autoregressive Transformer</h1> <p>Figure is from https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT, all rights reserve to its owner</p> </d-title><d-article> <p>Paper link: <a href="https://arxiv.org/abs/2401.04577" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2401.04577</a><br> <strong>Homepage</strong>: <a href="https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT/" rel="external nofollow noopener" target="_blank"> https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT</a> <br></p> <h2 id="d1">D1</h2> <p>The overall idea of the MagNET is that gradually generate discrete tokens given already generated tokens. Those tokens are then converted into audio waveform using pretrained codecs. The mask and generate idea has been proposed in image generation before, e.g the MaskGIT <d-cite key="chang2022maskgit"></d-cite> which gives a good introduction about the mask and generation procedure. It is competitive in the inference speech compared with diffusion based models.</p> <h3 id="background">Background</h3> <p><mark>MaskGIT</mark></p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/magnet/magnet-maskgit-pipline-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/magnet/magnet-maskgit-pipline-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/magnet/magnet-maskgit-pipline-1400.webp"></source> <img src="/assets/img/1Paper-7D/magnet/magnet-maskgit-pipline.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Above figure gives a easy-to-understand pipeline about the MaskGIT <d-cite key="chang2022maskgit"></d-cite>. The work presented in this paper has many aspect similar to it. We can borrow the figure to have a good understanding.</p> <p>The pipeline has two stages. First stage is called <strong>tokenization</strong> and it is to train a tokenizer which converts the original image into discrete tokens. It usually uses the encoder-decoder structure and auto-encoder is usually used. The second stage is refered as masked modeling. During inference, it starts from a blank canvas with all the tokens masked out. The four steps used:</p> <ol> <li>Predict: predict probabilities of token codebook,</li> <li>Sample: sample token based on its prediction probabilities,</li> <li>Mask schedule: compute the number of tokens to mask again,</li> <li>Mask.</li> </ol> <p><mark>Text condition</mark> A follow-up paper of MaskGIT is Muse<d-cite key="chang2023muse"></d-cite>.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/magnet/muse-overview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/magnet/muse-overview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/magnet/muse-overview-1400.webp"></source> <img src="/assets/img/1Paper-7D/magnet/muse-overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><mark>Audio representation</mark> Two types of representation are prevaling. The first type is dicrete representation (token). This type of representation are applied in many neural networks based codec, such as EnCodec. Specifically, auto-encoder is used to map waveform into a latent space and Residual Vector Quantization (RVQ) is used to convert continuous latent representation into discrete tokens. Those discrete tokens can be manipulated (e.g. transiting) and then can be converted to waveform through decoder component of the auto-encoder.</p> <h2 id="method">Method</h2> <p>Authors find it is practically not feasible to naively apply methods such in MaskGIT and Muse into the RVQ represented audio tokens. Authors hypothesize it is due to:</p> <ol> <li>individual token in audio shares information with adjacent tokens,</li> <li>the first code in codebook usually encode most information and the temporal context of the codebooks at levels greater than one is generally local and influenced by a small set of neighboring tokens,</li> <li>sampling from the model at different decoding steps requires different levels of diversity with respect to the condition. (<mark>I don't understand how this aspect is special for audio modality</mark>)</li> </ol> <p><mark>Masking span instead of single token</mark>. The author’s solution one is applying spans of tokens as the atomic building block of the masking scheme, rather than indivisual one.</p> <p><mark>restricting context</mark>. The quantized codebooks later than the first one heavily depend on previous codebooks rather than surrounding tokens. The autho put prior knowledge (inductive bias) into the attention map of those codebooks attention weights inside the transformer to limit the context modeling, which helps narrow down the optimization process.</p> <p><mark>rescoring</mark>. Author inspired from Automatic Speech Recognition (ASR) decoding, that usually generates several decoding candidates and resore them using another model. Authors propose to use an external model to evaluate and precit new set of probabilities of each token spans. Then weighted sum of both MagNET’s probabilities and rescorer model’s probabilities is used. More specifically, MUSICGEN and AudioGen are used as rescorering models.</p> <p><mark>Classifier-free guidance annealing</mark>. Authors decreases the impactness of the condition while generating audio, which means the tokens generated first will be more impacted by condition. Similar as Calssifier-Free Guidance (CFG) in diffusion-based model, during training, authors optimize the model both conditionally and unconditionally, while at inference, distribution as a linear combination of the conditional and unconditional probabilities are used to sample the tokens. The CFG coefficient is specificied in Eq. (6).</p> <h2 id="comments">Comments</h2> <p>Authors make an attempt to use the mask token modeling (an analog of mask language model in NLP) in audio generation and show it works. The work is novelty in audio genration, though similar ideas were explored in image generation. Many of the developments are quite empirical, thus the ablation study is critical. The exteral rescorer models using MUSICGEN and AudioGen makes the model complicated. Does it really necessary to be included in the paper? Maybe put it into the appendix is a better way.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/magnet/ziv2024masked-ablation-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/magnet/ziv2024masked-ablation-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/magnet/ziv2024masked-ablation-1400.webp"></source> <img src="/assets/img/1Paper-7D/magnet/ziv2024masked-ablation.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-01-31-ziv2024masked.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Jianbo Ma. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>