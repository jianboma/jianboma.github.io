---
layout: distill
title: Variational auto-encoder
date: 2024-01-08
description: VAE
tags: DL
categories: education
bibliography: for_vae.bib
giscus_comments: false
related_posts: false

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
# toc:
#   - name: TL;DR
#   - name: D1
#     subsections:
#       - name: State Space Model
#   - name: D2
#     subsections:
#     - name: HiPPO
#     - name: Discrete-time SSM
#   - name: D3
#     subsections:
#     - name: Convolutional representation
#     - name: S4 algorithms



# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---

# Resources of LLMs
This is a collection of resources.

## Models

Meta:
Llama, Llama 2

Microsoft:
phi series 
the phi-1 paper: textbooks are all you need
phi-2

Mistral 7B, Mistral 8x-7B (mixture of expert, only works on FFN of transformer)
Zephyr (CPO on top of Mistral 7B)

## Auto-encoder



