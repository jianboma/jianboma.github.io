---
layout: distill
title: A perspective of transformer
date: 2023-12-15
description: a perspective
tags: perspective_share
categories: paper_read
giscus_comments: false
related_posts: false

# authors:
#   - name: Albert Einstein
    # url: "https://en.wikipedia.org/wiki/Albert_Einstein"
    # affiliations:
    #   name: IAS, Princeton


bibliography: 2023-12-15-transformer-perspective.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Transformer

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---

## Transformer

Since the original `transformer` <d-cite key="vaswani2017attention"></d-cite> paper has been published in 2017, the `transformer` is the most popular building block of many tasks in deep learning, at least it is the most popular one in NLP. To get sense of transformer, this [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) should be the best place to get started.

Previously, Recurrent Neural Networks (RNNs) such as LSTM is the popular building block of NLP, but the major drawback of RNN is that the training is slow due to the state-dependency. This makes the RNN is hard to scale to large amount of data and is hard to take advantage of large amount of processors in hardware. The transformer solves the problem by allowing each state to interate with any other states available without any prior assumption (no inductive bias). The relative importances of different states are determined on-the-fly by the `self-attention`.

In addition to be popular in NLP, many other tasks such as computer vision and audio areas. The Vision Transformer (ViT) <d-cite key="dosovitskiy2020image"></d-cite> compares the ViT with CNNs architectures in CV and ViT becomes one of the options in CV as well. Other areas, such as audio, transformer is already become one of the most popular buidling block across many different tasks, e.g. ASR <d-cite key="radford2023robust,chiu2018state"></d-cite>.


## Some Notes

flash attention
efficient attention
linear transformer
