<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Read Robust Speech Recognition via Large-Scale Weak Supervision | Jianbo Ma</title> <meta name="author" content="Jianbo Ma"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/profile_head3.png?827aece6b42572b4a67200cce7342aee"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jianboma.github.io/projects/1Paper-7D/2024-01-22-whisper/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Read Robust Speech Recognition via Large-Scale Weak Supervision",
      "description": "",
      "published": "January 22, 2024",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">JianboÂ </span>Ma</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/blog/">Blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Read Robust Speech Recognition via Large-Scale Weak Supervision</h1> <p></p> </d-title><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#d1">D1</a></div> <ul> <li><a href="#motivation">Motivation</a></li> <li><a href="#models-and-tasks">Models and tasks</a></li> <li><a href="#"></a></li> </ul> <div><a href="#d2">D2</a></div> <ul> <li><a href="#zero-shot">Zero-shot</a></li> </ul> </nav> </d-contents> <p>Paper link: <a href="https://arxiv.org/pdf/2212.04356.pdf" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2212.04356.pdf</a><br> <strong>Homepage</strong>: <a href="https://github.com/openai/whisper" rel="external nofollow noopener" target="_blank"> https://github.com/openai/whisper</a> <br></p> <h2 id="d1">D1</h2> <h3 id="motivation">Motivation</h3> <p>From the introduction, the motivation of this paper is to how to scale up the data usage with a supervised mannual.</p> <p>Unsupervised pre-training techniques like Wav2Vec 2.0 <d-cite key="baevski2020wav2vec"></d-cite> can take advantage of large amount of unlabeled audio data. It can easily scale up to 1000000 hours of training data <d-cite key="zhang2022bigssl"></d-cite>.</p> <p>Authors reason there are drawbacks of the previous unsupervised pre-training techniques:</p> <ul> <li>previous unsupervised pre-training focus on trainig an generic audio encoder, but not the decoder.</li> <li>decoder is task specific.</li> <li>there are risk of fine-tuning. Pre-trained model within a training data may not generalize well to held-out data <code class="language-plaintext highlighter-rouge">as some of patterns are brittle and spurious and don't generalize to other datasets and distribution</code>.</li> <li>this work is on studying the capabilities of large-scale supervised pre-training for speech recognition.</li> </ul> <p>The reasoning in the introduction is not that sound for me, as this seems to be general question for all unsupervised pre-training. It is reasonable to train with supervised training with the same amount of data compared with unsupervised training. Given a task with large amount of labeled data, supervised training is still prefered.</p> <h3 id="models-and-tasks">Models and tasks</h3> <p>The overview figure (figure 1 in paper) provides a good explanation of the model and multi-task training procedure.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/radford2023robust/whisper-overview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/radford2023robust/whisper-overview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/radford2023robust/whisper-overview-1400.webp"></source> <img src="/assets/img/1Paper-7D/radford2023robust/whisper-overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><mark>Model Architecture</mark></p> <p>The structure of the model is a typical Attention Encoder-Decoder (AED) model, which starts from the <code class="language-plaintext highlighter-rouge">listen, attend and spell</code> paper <d-cite key="chan2016listen"></d-cite> by replacing the RNNs inside with transformers. There are more sophisticated architectures, such as RNN-Transducer. <strong>The main reason authors choose it because AED is a fast to parallel, which means can easily scale up to more data and this is the core idea of the paper.</strong></p> <p><mark>Multi-task format</mark></p> <p>Authors include tasks: transcription, translation, voice activity detection, alignment, and language identification. But not speaker identification. The precedure is:</p> <ol> <li>include previous transcription?</li> <li>begin token &lt;|startoftranscript|&gt;</li> <li>predict language target</li> <li>if there is no speech, add &lt;|nospeech|&gt; token</li> <li>specify task: &lt;|transcribe|&gt; or &lt;|translate|&gt;</li> <li>whether to predict timestamps: &lt;|notimestaps|&gt; for not predicting timestamps</li> <li>after decoded, add &lt;|endoftranscript|&gt;</li> </ol> <p>Feel like a pseudo code block is good to be included in the paper.</p> <h2 id="d2">D2</h2> <h3 id="zero-shot">Zero-shot</h3> <p>The main target of this paper is to show with large amount of data that is various in many aspect, speech recognition system can generalize well, which mean it performs well in zero-shot situation.</p> <details> <summary>About the zero-shot claim</summary> I still find the zero-shot setting is not really 'zero-shot'. This zero-shot is some what strange for me. The 'zero-shot' here is more about a specific dataset for testing is not included in the training. But this does not mean there is not overlap between the distribution of the specific dataset with training dataset. </details> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/radford2023robust/whisper-zero-shot-figure-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/radford2023robust/whisper-zero-shot-figure-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/radford2023robust/whisper-zero-shot-figure-1400.webp"></source> <img src="/assets/img/1Paper-7D/radford2023robust/whisper-zero-shot-figure.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/radford2023robust/whisper-zero-shot-table-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/radford2023robust/whisper-zero-shot-table-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/radford2023robust/whisper-zero-shot-table-1400.webp"></source> <img src="/assets/img/1Paper-7D/radford2023robust/whisper-zero-shot-table.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>The above two figures summarize the zero-shot performance. Authors use the overall robustness, that is average performance across many distributions/datasets, and effective robustness <d-cite key="taori2020measuring"></d-cite>, which measures the difference in expected performance between a reference dataset, which is usually in-distribution, and one or more out-of-distribution datasets.</p> <p><mark>Multilingual</mark> Another finding they have is in multiligual speech recognition performance. The whisper is not superior in Multilingual LibriSpeech and VoxPopuli. Author find a strong squared correlation coefficient of 0.83 between the log of the word error rate and the log of the amount of training data per language, which results in an estimate that WER halves for ever 16x increase in training data. Many of the largest outliers in terms of worse than expected performance are languages that have unique scripts and are more distantly related to the Indo-European languages such as Hebrew (HE), Telugu (TE), Chinese (ZH), and Korean (KO). This observation is interesting but I found is empirical. Authors suspect the reason is:</p> <blockquote> <p>These differences could be due to a lack of transfer due to linguistic distance,our byte level BPE tokenizer being a poor match for these languages, or variations in data quality.</p> </blockquote> <p><mark>Translation</mark> Authors find the translation from lanugages to english is better than prior work like Maestro, mSLAM, and XLS-R in the subset of CoVOST2. The attribute this to the 68 thousand hours of x-&gt;en translation data for these languages in pre-training dataset which, although noisy, is vastly larger than the 861 hours of training data fro x-&gt;en translation in CoVoST2.</p> <p>The squared corrlation coefficient between the amount of translation training data per language and the resulting zero-shot BLEU score on Fleurs is much lower than the 0.83 observed for speech recogntiion and is only 0.24.</p> <p><mark>Language Identification</mark> Fleurs dataset is used. The zero-shot performance of Whisper is not competitive with prior supervised work and underperforms the supervised SOTA by 13.6%. This is partly due to there are 20 out of 102 languages in Fleurs are not in the training data of Whisper. The best whisper model achieves 80.3% accuracy on the other 82 languages.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-01-22-radford2023robust.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2024 Jianbo Ma. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>