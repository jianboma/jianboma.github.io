---
layout: distill
title: Read Robust Speech Recognition via Large-Scale Weak Supervision
description: 
img: assets/img/1Paper-7D/radford2023robust/whisper-overview.png
importance: 1
category: 1Paper-7D
bibliography: 2024-01-22-radford2023robust.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: D1
    subsections:
    - name: Motivation
    - name: Models and tasks
    - name: 
  - name: D2
    subsections:
    - name: Zero-shot
#     - name: practical guide for NLP task
#     - name: practical guide for generation task
#     - name: knowledge intensive tasks
#   - name: D3
#     subsections:
#     - name: Scaling
#     - name: Miscellaneous tasks


# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---

<!-- #  -->
Paper link: [https://arxiv.org/pdf/2212.04356.pdf](https://arxiv.org/pdf/2212.04356.pdf)<br>
**Homepage**: [ https://github.com/openai/whisper]( https://github.com/openai/whisper) <br>


<!-- ## TL;DR -->

## D1

### Motivation
From the introduction, the motivation of this paper is to how to scale up the data usage with a supervised mannual. 

Unsupervised pre-training techniques like Wav2Vec 2.0 <d-cite key="baevski2020wav2vec"></d-cite> can take advantage of large amount of unlabeled audio data. It can easily scale up to 1000000 hours of training data <d-cite key="zhang2022bigssl"></d-cite>.

Authors reason there are drawbacks of the previous unsupervised pre-training techniques:
- previous unsupervised pre-training focus on trainig an generic audio encoder, but not the decoder.
- decoder is task specific.
- there are risk of fine-tuning. Pre-trained model within a training data may not generalize well to held-out data `as some of patterns are brittle and spurious and don't generalize to other datasets and distribution`.
- this work is on studying the capabilities of large-scale supervised pre-training for speech recognition.

The reasoning in the introduction is not that sound for me, as this seems to be general question for all unsupervised pre-training. It is reasonable to train with supervised training with the same amount of data compared with unsupervised training. Given a task with large amount of labeled data, supervised training is still prefered.

### Models and tasks
The overview figure (figure 1 in paper) provides a good explanation of the model and multi-task training procedure.
<div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/1Paper-7D/radford2023robust/whisper-overview.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<mark>Model Architecture</mark>

The structure of the model is a typical Attention Encoder-Decoder (AED) model, which starts from the `listen, attend and spell` paper <d-cite key="chan2016listen"></d-cite> by replacing the RNNs inside with transformers. There are more sophisticated architectures, such as RNN-Transducer. **The main reason authors choose it because AED is a fast to parallel, which means can easily scale up to more data and this is the core idea of the paper.**

<mark>Multi-task format</mark>

Authors include tasks: transcription, translation, voice activity detection, alignment, and language identification. But not speaker identification. 
The precedure is: 
1. include previous transcription?
2. begin token <&#124;startoftranscript&#124;>
3. predict language target
4. if there is no speech, add <&#124;nospeech&#124;> token
5. specify task: <&#124;transcribe&#124;> or <&#124;translate&#124;>
6. whether to predict timestamps: <&#124;notimestaps&#124;> for not predicting timestamps
7. after decoded, add <&#124;endoftranscript&#124;>

Feel like a pseudo code block is good to be included in the paper.

## D2

### Zero-shot
The main target of this paper is to show with large amount of data that is various in many aspect, speech recognition system can generalize well, which mean it performs well in zero-shot situation. 
<details>
<summary>About the zero-shot claim</summary>
I still find the zero-shot setting is not really 'zero-shot'. This zero-shot is some what strange for me. The 'zero-shot' here is more about a specific dataset for testing is not included in the training. But this does not mean there is not overlap between the distribution of the specific dataset with training dataset.
</details>
<div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/1Paper-7D/radford2023robust/whisper-zero-shot-figure.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/1Paper-7D/radford2023robust/whisper-zero-shot-table.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
The above two figures summarize the zero-shot performance. Authors use the overall robustness, that is average performance across many distributions/datasets, and effective robustness <d-cite key="taori2020measuring"></d-cite>, which measures the difference in expected performance between a reference dataset, which is usually in-distribution, and one or more out-of-distribution datasets.

<mark>Multilingual</mark>
Another finding they have is in multiligual speech recognition performance. The whisper is not superior in Multilingual LibriSpeech and VoxPopuli. Author find a strong squared correlation coefficient of 0.83 between the log of the word error rate and the log of the amount of training data per language, which results in an estimate that WER halves for ever 16x increase in training data. Many of the largest outliers in terms of worse than expected performance are languages that have unique scripts and are more distantly related to the Indo-European languages such as Hebrew (HE), Telugu (TE), Chinese (ZH), and Korean (KO). This observation is interesting but I found is empirical. Authors suspect the reason is:
> These differences could be due to a lack of transfer due to linguistic distance,our byte level BPE tokenizer being a poor match for these languages, or variations in data quality.

<mark>Translation</mark>
Authors find the translation from lanugages to english is better than prior work like Maestro, mSLAM, and XLS-R in the subset of CoVOST2. The attribute this to the 68 thousand hours of x->en translation data for these languages in pre-training dataset which, although noisy, is vastly larger than the 861 hours of training data fro x->en translation in CoVoST2.

The squared corrlation coefficient between the amount of translation training data per language and the resulting zero-shot BLEU score on Fleurs is much lower than the 0.83 observed for speech recogntiion and is only 0.24.

<mark>Language Identification</mark>
Fleurs dataset is used. The zero-shot performance of Whisper is not competitive with prior supervised work and underperforms the supervised SOTA by 13.6%. This is partly due to there are 20 out of 102 languages in Fleurs are not in the training data of Whisper. The best whisper model achieves 80.3% accuracy on the other 82 languages.

