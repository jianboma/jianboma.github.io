<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Read Harnessing the power of llms in practice A survey on chatgpt and beyond | Jianbo Ma</title> <meta name="author" content="Jianbo Ma"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/profile_head3.png?827aece6b42572b4a67200cce7342aee"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jianboma.github.io/projects/1Paper-7D/2024-01-15-harnessing-the-power-of-llms-in-practice/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Read Harnessing the power of llms in practice A survey on chatgpt and beyond",
      "description": "",
      "published": "January 15, 2024",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jianbo </span>Ma</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/blog/">Blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Read Harnessing the power of llms in practice A survey on chatgpt and beyond</h1> <p></p> </d-title><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#d1-practical-guide-for-models">D1 practical guide for models</a></div> <div><a href="#d2">D2</a></div> <ul> <li><a href="#practical-guide-for-data">practical guide for data</a></li> <li><a href="#practical-guide-for-nlp-task">practical guide for NLP task</a></li> <li><a href="#practical-guide-for-generation-task">practical guide for generation task</a></li> <li><a href="#knowledge-intensive-tasks">knowledge intensive tasks</a></li> </ul> <div><a href="#d3">D3</a></div> <ul> <li><a href="#scaling">Scaling</a></li> <li><a href="#miscellaneous-tasks">Miscellaneous tasks</a></li> </ul> </nav> </d-contents> <p>Paper link: <a href="https://arxiv.org/abs/2111.00396" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2304.13712.pdf</a><br> <strong>Homepage</strong>: <a href="https://github.com/Mooler0410/LLMsPracticalGuide" rel="external nofollow noopener" target="_blank"> https://github.com/Mooler0410/LLMsPracticalGuide</a> <br> This is a survey paper and it’s included because it gives good overview of the current Large Language Model (LLM).</p> <h2 id="d1-practical-guide-for-models">D1 practical guide for models</h2> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-evolutionary-tree-of-modern-LLMs-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-evolutionary-tree-of-modern-LLMs-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-evolutionary-tree-of-modern-LLMs-1400.webp"></source> <img src="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-evolutionary-tree-of-modern-LLMs.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Authors divided the LLMs into two categories: <code class="language-plaintext highlighter-rouge">encoder-decoder or encoder-only</code>, <code class="language-plaintext highlighter-rouge">decoder-only</code>. The observation from the author is that the <code class="language-plaintext highlighter-rouge">decoder-only</code> is dominant in the field, and the encoder-only models gradually fade away after BERT.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-tables-of-LLMs-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-tables-of-LLMs-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-tables-of-LLMs-1400.webp"></source> <img src="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-tables-of-LLMs.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Some models:</p> <ul> <li>BERT <d-cite key="devlin2018bert"></d-cite>, proposed the Masked Language Models (MLM) that predict the masked region considering the surrounding context.</li> <li>GPT-3 <d-cite key="brown2020language"></d-cite>, using the Autoregressive alanuage Models (predict next word using preceding words), demonstrated reasonable few-/zero-shot performance via prompting and in-context learning.</li> </ul> <blockquote> <p>the definitions of them are proposed as: <strong>LLMs</strong> are huge language models pretrained on large amounts of datasets without tuning on data for specific tasks; <strong>fine-tuned models</strong> are typically smaller language models which are also pretrained and then further tuned on a smaller, task-specific dataset to optimize their performance on that task.</p> </blockquote> <details> <summary>footnote</summary> From a practical standpoint, we consider models with less than 20B parameters to be fine-tuned models. While it’s possible to fine-tune even larger models like PlaM (540B), in reality, it can be quite challenging, particularly for academic research labs and small teams. Fine-tuning a model with 3B parameters can still be a daunting task for many individuals or organizations. </details> <h2 id="d2">D2</h2> <h3 id="practical-guide-for-data">practical guide for data</h3> <p>LLMs at least have two stages: pre-training and fine-tuning stage.</p> <ul> <li> <strong>pretraining data</strong>: this data contains large amount of categories with huge amount of data for pretraining. It includes books, articles, and websites. The quality, quantitative and diversity of this data inflence the performance of LLMs significantly. <code class="language-plaintext highlighter-rouge">the selection of LLMs highly depends on the components of the pretraining data ... code execution and code completion capabilities of GPT-3.5 (code-davinci-002) are amplified by the integration of code data in its pretraining dataset. In brief, when selecting LLMs for downstream tasks, it is advisable to choose the model pre-trained on a similar field of data.</code> </li> <li> <strong>fine-tuning data</strong>: the author further divided it into three scenario: <ul> <li> <mark>zero annotated data</mark>: there is not much information in the manuscript. It just gives praise to the zero-shot performance of LLMs.</li> <li> <mark>few annotated data</mark>: Few annotated data is quite useful. The few-shot examples are directly incorporated in the input prompt of LLMs. The <code class="language-plaintext highlighter-rouge">in-context learning</code> is effective to make the LLMs to generalize to the task.</li> <li> <mark>abundant annotated data</mark>: task specific. Both fine-tuned models and LLMs can be considered. Fine-tuning the model might be cheaper.</li> </ul> </li> </ul> <h3 id="practical-guide-for-nlp-task">practical guide for NLP task</h3> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-nlp-task-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-nlp-task-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-nlp-task-1400.webp"></source> <img src="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-nlp-task.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <blockquote> <p>Fine-tuned models generally are a better choice than LLMs in traditional NLU tasks, but LLMs can provide help while requiring strong generalization ability.</p> </blockquote> <p>Tasks:</p> <ul> <li>text classification, datasets: IMDB, SST. Toxicity detection (fine-tuned models are far more better than LLMs), CivilComments. Perspective API is still one of the best for detecting toxicity. This API is powered by a multilingual BERT-based model.</li> <li>natural language inference, datasets: RTE, SNLI (fine-tuned models are better). For question answering (QA), datasets: SQuADv2, AuAC, fin-tuned moels are better. CoQA LLs performs as well.</li> <li>information retrieval (IR), KKNs are not well exploited. Ranking thousands of candidate texts seems not easy fo LLMs.</li> <li>low-level intermediate tasks like named entity recognition (NER). <mark>Authors predict those intermediate tasks may be skipped by LLMs as LLMs can achieve high-level tasks without the help of those intermediate results.</mark> </li> <li>tasks that has out-of-distribution and sparsely annotated data like Adversarial NLI (ANLI), miscellaneous text classification, LLMs are better.</li> </ul> <h3 id="practical-guide-for-generation-task">practical guide for generation task</h3> <blockquote> <p>LLM can perform summarization and translation tasks that generate results human perfer. LLMs can perform competent translation, particularly good at translating some low-resource language texts to English texts, such as in the Romanian-English translation of WMT’16…</p> </blockquote> <ul> <li>good at open-ended generation, like news article writing</li> <li>good at code synthesis, like text-code generation, such as HumannEval<d-cite key="chen2021evaluating"></d-cite> and MBPP, or for code repairing, such as DeepFix, LLM perform well. GPT-4 can even pass 25% problems in Leetcode<d-cite key="achiam2023gpt"></d-cite>, which are not trivial for most human coders. With training on more code data, the coding capability of LLMs can be improved further [PaLM]<d-cite key="chowdhery2023palm"></d-cite>.</li> </ul> <details> <summary>PaLM</summary> Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM). We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies. </details> <h3 id="knowledge-intensive-tasks">knowledge intensive tasks</h3> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-knowledge-intensive-task-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-knowledge-intensive-task-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-knowledge-intensive-task-1400.webp"></source> <img src="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-knowledge-intensive-task.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Use: tasks that need real-world knowledge, like question-answering, multitask language understanding etc.</p> <p>No use: tasks requiring knowledge different from one learned by LLMs or only contextual knowledge during inference is needed.</p> <h2 id="d3">D3</h2> <h3 id="scaling">Scaling</h3> <p>The ‘scaling-law’ <d-cite key="kaplan2020scaling"></d-cite> is applicable to LLMs and can greatly empower pretrained language models. With the model scaling up, a model generally becomes more capable in a range of tasks.</p> <details> <summary>Scaling law</summary> We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence. The key findings for Transformer language models are: <ul> <li> <b>Performance depends on scale, weakly on model shape</b>: the number of model parameters N (excluding embeddings), the size of the dataset D, and the amount of compute C used for training. Has weak depends on other architectural hyperparameters such as depth and width. </li> <li> <b>Smooth power laws</b>: performance has a power-law relationship with each of the three factors N, D, C.</li> <li> <b>Universality of overfitting</b>: Performance improves predictably as long as we scale up N and D in tandem, but enters a regime of diminishing returns if either N or D is held fixed while the other increases. The performance penalty depends predictably on the ratio N^0.74/D, meaning that every time we increase the model size 8x, we only need to increase the data by roughly 5x to avoid a penalty</li> <li> <b>Universality of training</b>:Training curves follow predictable power-laws whose parameters are roughly independent of the model size. By extrapolating the early part of a training curve, we can roughly predict the loss that would be achieved if we trained for much longer. </li> <li> <b>Transfer improves with test performance</b>:(I did not understand). </li> <li> <b>Sample efficiency</b>(The amount of information an algorithm can get from samples. )&lt;/d-footnote&gt;:(I did not understand). </li> <li> <b>Convergence is inefficient</b>: When working within a fixed compute budget C but without any other restrictions on the model size N or available data D, we attain optimal performance by training very large models and stopping significantly short of convergence . </li> <li> <b>Optimal batch size</b>:(I did not understand). </li> </ul> Taken together, these results show that language modeling performance improves smoothly and predictably as we appropriately scale up model size, data, and compute. We expect that larger language models will perform better and be more sample efficient than current models. </details> <ul> <li>Arithmetic reasoning increase when scaling-up but can still occasionally make mistakes.</li> <li>Commensense reasoning gradually increase with the growth of model size.</li> <li>Emergent ability<d-cite key="wei2022emergent"></d-cite><d-footnote>emergent abilities of LLMs are abilities that are not present in smaller-scale models but are present in large-scale models.</d-footnote>: it is not predictable. Some emergent abilities are interesting, such as handling word manipulation, logical abilities etc. Research needed to understand more and the U-shaped Phenomenon<d-cite key="wei2022inverse"></d-cite> </li> </ul> <h3 id="miscellaneous-tasks">Miscellaneous tasks</h3> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-miscellaneous-task-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-miscellaneous-task-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-miscellaneous-task-1400.webp"></source> <img src="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-miscellaneous-task.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ul> <li> <strong>No use case</strong>: <blockquote> <p>LLMs performance in regression tasks has been less impressive. For example, ChatGPT’s performance on the GLUE STS-B dataset, which is a regression task evaluating sentence similarity, is inferior to a fine-tuned RoBERTa performance. LLMs’ performance on multimodal data, which involves handling multiple data types such as text, images, audio, video, actions, and robotics, remains largely unexplored. And fine-tuned multimodal models, like BEiT and PaLI, still dominate many tasks such as visual question answering (VQA) and image captioning. (This seems to be out dated informaiton.)</p> </blockquote> </li> <li> <strong>Use case</strong>: mimichking humans such as ChatGPT, annotator and data generator for data augmentation, quality assessment on some NLG tasks. </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-01-15-yang2023harnessing.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Jianbo Ma. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>