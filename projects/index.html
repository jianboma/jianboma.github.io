<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Projects | Jianbo Ma</title> <meta name="author" content="Jianbo Ma"> <meta name="description" content="Describe projects"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/profile_head3.png?827aece6b42572b4a67200cce7342aee"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jianboma.github.io/projects/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jianbo </span>Ma</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/blog/">Blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Projects</h1> <p class="post-description">Describe projects</p> </header> <article> <h2 id="1paper-7d">1Paper-7D</h2> <p>In this category, I am going to read a paper in detail over a week.</p> <p>在这个类别里，我将在一周的时间里详细的阅读一篇论文。详细记录理解，问题和想法。</p> <div class="projects"> <h2 class="category">1Paper-7D</h2> <div class="grid"> <div class="grid-sizer"></div> <div class="grid-item"> <a href="/projects/1Paper-7D/2023-12-11-rwkv/"> <div class="card hoverable"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-icon-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-icon-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/rwkv/rwkv-icon-1400.webp"></source> <img src="/assets/img/1Paper-7D/rwkv/rwkv-icon.png" width="auto" height="auto" alt="project thumbnail" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="card-body"> <h2 class="card-title text-lowercase">Read `RWKV Reinventing RNNs for the Transformer Era`</h2> <p class="card-text">the icon is from https://wiki.rwkv.com/, all rights reserves to its onwer.</p> <div class="row ml-1 mr-1 p-0"> </div> </div> </div> </a> </div> <div class="grid-sizer"></div> <div class="grid-item"> <a href="/projects/1Paper-7D/2023-12-18-structured_state_spaces/"> <div class="card hoverable"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/ssm_long_sequence/ssm-overview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/ssm_long_sequence/ssm-overview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/ssm_long_sequence/ssm-overview-1400.webp"></source> <img src="/assets/img/1Paper-7D/ssm_long_sequence/ssm-overview.png" width="auto" height="auto" alt="project thumbnail" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="card-body"> <h2 class="card-title text-lowercase">Read `efficiently modeling long sequences with stuctured state spaces`</h2> <p class="card-text"></p> <div class="row ml-1 mr-1 p-0"> </div> </div> </div> </a> </div> <div class="grid-sizer"></div> <div class="grid-item"> <a href="/projects/1Paper-7D/2024-01-15-harnessing-the-power-of-llms-in-practice/"> <div class="card hoverable"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-overview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-overview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-overview-1400.webp"></source> <img src="/assets/img/1Paper-7D/yang2023harnessing/yang2023harnessing-overview.png" width="auto" height="auto" alt="project thumbnail" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="card-body"> <h2 class="card-title text-lowercase">Read Harnessing the power of llms in practice A survey on chatgpt and beyond</h2> <p class="card-text"></p> <div class="row ml-1 mr-1 p-0"> </div> </div> </div> </a> </div> <div class="grid-sizer"></div> <div class="grid-item"> <a href="/projects/1Paper-7D/2024-01-22-whisper/"> <div class="card hoverable"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/radford2023robust/whisper-overview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/radford2023robust/whisper-overview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/radford2023robust/whisper-overview-1400.webp"></source> <img src="/assets/img/1Paper-7D/radford2023robust/whisper-overview.png" width="auto" height="auto" alt="project thumbnail" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="card-body"> <h2 class="card-title text-lowercase">Read Robust Speech Recognition via Large-Scale Weak Supervision</h2> <p class="card-text"></p> <div class="row ml-1 mr-1 p-0"> </div> </div> </div> </a> </div> <div class="grid-sizer"></div> <div class="grid-item"> <a href="/projects/1Paper-7D/2024-01-31-magnet/"> <div class="card hoverable"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1Paper-7D/magnet/magnet-method-overview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1Paper-7D/magnet/magnet-method-overview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1Paper-7D/magnet/magnet-method-overview-1400.webp"></source> <img src="/assets/img/1Paper-7D/magnet/magnet-method-overview.png" width="auto" height="auto" alt="project thumbnail" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="card-body"> <h2 class="card-title text-lowercase">Masked Audio Generation using a Single Non-Autoregressive Transformer</h2> <p class="card-text">Figure is from https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT, all rights reserve to its owner</p> <div class="row ml-1 mr-1 p-0"> </div> </div> </div> </a> </div> </div> <h2 class="category">Fast-read</h2> <div class="grid"> <div class="grid-sizer"></div> <div class="grid-item"> <a href="/projects/fast_read/2024-02-06-olmo/"> <div class="card hoverable"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/quick-paper-share/olmo-icon-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/quick-paper-share/olmo-icon-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/quick-paper-share/olmo-icon-1400.webp"></source> <img src="/assets/img/quick-paper-share/olmo-icon.png" width="auto" height="auto" alt="project thumbnail" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="card-body"> <h2 class="card-title text-lowercase">OLMo - Accelerating the Science of Language Models</h2> <p class="card-text">The figure is from https://github.com/allenai/OLMo. All rights reserved to its owner.</p> <div class="row ml-1 mr-1 p-0"> </div> </div> </div> </a> </div> </div> <h2 class="category">work</h2> <div class="grid"> </div> <h2 class="category">fun</h2> <div class="grid"> </div> </div> <h2 id="candidate-papers-to-read">Candidate papers to read</h2> <ul class="task-list"> <li class="task-list-item"> <p><input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://arxiv.org/abs/2312.00752" rel="external nofollow noopener" target="_blank">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a> <details> &lt;summary&gt;Details&lt;/summary&gt; <strong>Authors</strong>: Albert Gu, Tri Dao <br> <strong>Date</strong>: 1 Dec 2023 <br> <strong>#Citations</strong>: 0 <br> <code> @article{gu2023mamba, title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, author={Gu, Albert and Dao, Tri}, journal={arXiv preprint arXiv:2312.00752}, year={2023} } </code><br> <strong>Details</strong>: Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers’ computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5× higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. </details></p> </li> <li class="task-list-item"> <p><input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://arxiv.org/abs/2111.06377" rel="external nofollow noopener" target="_blank">Masked autoencoders are scalable vision learners</a> <details> &lt;summary&gt;Details&lt;/summary&gt; <strong>Authors</strong>: Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick <br> <strong>Date</strong>: 11 Nov 2021 <br> <strong>#Citations</strong>: 3719 <br> <code> @inproceedings{he2022masked, title={Masked autoencoders are scalable vision learners}, author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross}, booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages={16000--16009}, year={2022} } </code><br> <strong>Details</strong>: This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior. </details></p> </li> <li class="task-list-item"> <p><input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://arxiv.org/abs/2311.17117" rel="external nofollow noopener" target="_blank">Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation</a></p> </li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Jianbo Ma. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>