---
layout: distill
title: OLMo - Accelerating the Science of Language Models
description: The figure is from https://github.com/allenai/OLMo. All rights reserved to its owner.
img: assets/img/quick-paper-share/olmo-icon.png
importance: 1
category: Fast-read
bibliography: for_rwkv.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: TL;DR
  - name: Architecture
  #   subsections:
  #     - name: Quadratic complexity
  #     - name: Background
  # - name: D2
  #   subsections:
  #   - name: Time mixing and channel mixing
  # - name: D3
  # - name: D5
  # - name: D6


# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---

Paper link: [https://allenai.org/olmo/olmo-paper.pdf](https://allenai.org/olmo/olmo-paper.pdf)<br>
**Homepage**: [https://github.com/allenai/OLMo](https://github.com/allenai/OLMo) <br>

## TL;DR
OLMo is short for **O**pen **L**anguage **Mo**del. As the name indicates, the main contribution as the authors believe is the truely openness of the language models. Authors argue that many Large Language Models (LLMs) are closed source as the commercial incentive. Though there are few models that are "open", such as LLaMa, Mistral 8$$\times$$7B, Mosaic, Falcon, Pythia suite, and BLOOM. Datasets used and training details are still hard to reproduce those models. 

OLMo intends to close the gap. It opens the datasets (including tools for generating the datasets), training logs, intermediate checkpoints, evaluation tools and evaluation datasets. This can shed the light for more researchers to reproduce the model and conduct research to better understand the LLM, though the computational resource is still a major obstacle.


## Architecture
<div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/fast_reading/olmo/olmo-architecture-t1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<mark>Questions:</mark><br>
1. What are the differences between encoder-decoder, encoder-only and decoder-only structrues?<br>
This might be an answer for this question. https://vaclavkosar.com/ml/Encoder-only-Decoder-only-vs-Encoder-Decoder-Transfomer

Encoder-only such as BERT, takes text as input and output a sequence of embeddings. Those embeddings are used for downstream tasks, such as sequence classification. Vision Transformer (ViT) is also encoder-only model.

Decoder-only arthitecture takes sequence of text as input and output the next word or token. This is used in many text generation task.

Encoder-decoder has input as text and output will be the next word or token, which is then appended to the decoder-input. In between, cross-attention usually is applied to make information passing efficiently. The encoder-decoder attention ASR is one of the example. This is prefered if the input and output are different, for example, different modalities, different languages etc.





